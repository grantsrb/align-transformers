{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "63ff6846",
   "metadata": {},
   "source": [
    "## Network Similarity"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "3ae11b28",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "__author__ = \"Satchel Grant and Daniel Wurgaft\"\n",
    "__version__ = \"11/05/2024\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d898fce",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "This tutorial aims to use RSA and MAS to measure model similarity. It uses the same pricing tag dataset as in [the Boundless DAS paper](https://arxiv.org/pdf/2305.08809). "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8af5dff0",
   "metadata": {},
   "source": [
    "## Set-up"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "3e3c09e2",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nnsight is not detected. Please install via 'pip install nnsight' for nnsight backend.\n"
     ]
    }
   ],
   "source": [
    "try:\n",
    "    # This library is our indicator that the required installs\n",
    "    # need to be done.\n",
    "    import pyvene\n",
    "\n",
    "except ModuleNotFoundError:\n",
    "    !pip install git+https://github.com/stanfordnlp/pyvene.git"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "9a39c2b3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from tqdm import tqdm, trange\n",
    "from datasets import Dataset\n",
    "from torch.utils.data import DataLoader\n",
    "from transformers import get_linear_schedule_with_warmup,AutoTokenizer, AutoModelForCausalLM\n",
    "from torch.nn import CrossEntropyLoss\n",
    "from tutorial_price_tagging_utils import (\n",
    "    factual_sampler,\n",
    "    pricing_tag_game_config_sampler,\n",
    "    bound_alignment_sampler,\n",
    "    lower_bound_alignment_example_sampler,\n",
    ")\n",
    "\n",
    "from pyvene import (\n",
    "    IntervenableModel,\n",
    "    BoundlessRotatedSpaceIntervention,\n",
    "    RepresentationConfig,\n",
    "    IntervenableConfig,\n",
    ")\n",
    "from pyvene import create_llama, create_gpt2\n",
    "from pyvene import set_seed, count_parameters\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "seed = 42\n",
    "torch.manual_seed(seed)\n",
    "np.random.seed(seed)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d61bb966-e3f5-4bf2-a3f3-26d3b6a57123",
   "metadata": {},
   "source": [
    "### Load Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "27a25265-5fa2-40c7-8a10-06797a00fd01",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def create_model(model_name=\"meta-llama/Llama-2-7b-hf\"):\n",
    "    tokenizer = AutoTokenizer.from_pretrained(model_name)\n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "       model_name, torch_dtype=torch.bfloat16, device_map=\"cuda\")\n",
    "    return None, tokenizer, model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "970a8f7d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/scr/wurgaft/miniconda3/envs/py310-wurgaft/lib/python3.12/site-packages/huggingface_hub/file_download.py:1142: FutureWarning: `resume_download` is deprecated and will be removed in version 1.0.0. Downloads always resume when possible. If you want to force a new download, use `force_download=True`.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74175cea90db473fa95f5d107d2e6288",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "You are using the default legacy behaviour of the <class 'transformers.models.llama.tokenization_llama.LlamaTokenizer'>. This is expected, and simply means that the `legacy` (previous) behavior will be used so nothing changes for you. If you want to use the new behaviour, set `legacy=False`. This should only be set if you understand what it means, and thoroughly read the reason why this was added as explained in https://github.com/huggingface/transformers/pull/24565\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3b9643b26e2c427f9fc37a42eee34a02",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Loading checkpoint shards:   0%|          | 0/34 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "loaded model\n"
     ]
    }
   ],
   "source": [
    "model_names = [ \"llama\", \"llama\", ]\n",
    "models = []\n",
    "tokenizers = []\n",
    "for model_name in model_names:\n",
    "    try:\n",
    "        _, tokenizer, model = create_model(model_name=model_name)\n",
    "    except: #TODO: make less hacky\n",
    "        _, tokenizer, model = create_llama()\n",
    "    \n",
    "    model.to(\"cuda\")  # single gpu\n",
    "    model.eval()  # always no grad on the model\n",
    "    models.append(model)\n",
    "    tokenizers.append(tokenizer)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "22bc3d1d-0093-4ad1-896b-42e05ff2e603",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "### Load task dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f7f5d9c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "dataloaders = []\n",
    "max_n_training_examples = 5000\n",
    "samples = [pricing_tag_game_config_sampler(None, None, None) for _ in range(max_n_training_examples)]\n",
    "for tokenizer in tokenizers:\n",
    "    raw_prealign = factual_sampler(tokenizer, max_n_training_examples, game=\"pricing_tag\", samples=samples)\n",
    "    prealign_dataset = Dataset.from_dict(\n",
    "        {\"input_ids\": raw_prealign[0], \"labels\": raw_prealign[1]}\n",
    "    )\n",
    "    prealign_dataset.set_format(\"torch\", columns=[\"input_ids\", \"labels\"])\n",
    "    prealign_dataloader = DataLoader(\n",
    "        prealign_dataset,\n",
    "        shuffle=False,\n",
    "        batch_size=8,\n",
    "        num_workers=0\n",
    "    )\n",
    "\n",
    "    dataloaders.append(prealign_dataloader)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa38e93b-fd35-471b-88fb-c0a5aad4469b",
   "metadata": {},
   "source": [
    "### Get hidden states and task accuracies from all models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e4cb38ae",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████████████████████████████████████████████████████| 625/625 [01:11<00:00,  8.75it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: <class 'transformers.models.llama.modeling_llama.LlamaForCausalLM'>\n",
      "[WARNING: THIS NEEDS TO BE GOOD!] prealign task accuracy: 0.91\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 94%|██████████████████████████████████████████████████████▎   | 585/625 [01:53<00:13,  2.96it/s]"
     ]
    }
   ],
   "source": [
    "hidden_states = { m: [] for m in models }\n",
    "with torch.no_grad():\n",
    "    for loop, (dloader,model) in enumerate(zip(dataloaders, models)):\n",
    "        total_count = 0\n",
    "        correct_count = 0\n",
    "        for step, inputs in enumerate(tqdm(dloader)):\n",
    "            for k, v in inputs.items():\n",
    "                if v is not None and isinstance(v, torch.Tensor):\n",
    "                    inputs[k] = v.to(model.device)\n",
    "    \n",
    "            # aligning forward!\n",
    "            outputs = model(\n",
    "                input_ids=inputs[\"input_ids\"],\n",
    "                labels=inputs[\"labels\"],\n",
    "                output_hidden_states=True,\n",
    "            )\n",
    "            \n",
    "            hidden_states[model].append(\n",
    "                [h.cpu().data for h in outputs.hidden_states]\n",
    "            )\n",
    "    \n",
    "            actual_test_labels = inputs[\"labels\"][:, -1]\n",
    "            pred_test_labels = torch.argmax(outputs.logits[:, -1], dim=-1)\n",
    "    \n",
    "            correct_labels = actual_test_labels == pred_test_labels\n",
    "    \n",
    "            total_count += len(correct_labels)\n",
    "            correct_count += correct_labels.sum().tolist()\n",
    "        current_acc = round(correct_count / total_count, 2)\n",
    "        print(\"Model:\", type(model))\n",
    "        print(f\"[WARNING: THIS NEEDS TO BE GOOD!] prealign task accuracy: {current_acc}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d7cdd5b-ae6f-4954-b739-046339cb63b9",
   "metadata": {},
   "source": [
    "## Run RSA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ee8f7325-31de-4862-b5e3-15d6639f006e",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# helper function\n",
    "def correlation_mtx(matrix):\n",
    "    matrix = matrix.float()\n",
    "    # Step 1: Center the matrix (subtract the mean of each column)\n",
    "    mean = matrix.mean(dim=0, keepdim=True)  # Mean for each column\n",
    "    centered_matrix = matrix - mean\n",
    "    # Step 2: Compute covariance matrix\n",
    "    cov_matrix = centered_matrix.T @ centered_matrix / (matrix.size(0) - 1)  # Unbiased covariance\n",
    "    # Step 3: Normalize by the standard deviations\n",
    "    std_dev = torch.sqrt(torch.diag(cov_matrix))\n",
    "    correlation_matrix = cov_matrix / (std_dev[:, None] * std_dev[None, :])  # Outer product of std_dev for normalization\n",
    "    # Print the resulting correlation matrix\n",
    "    return correlation_matrix\n",
    "\n",
    "\n",
    "def rsa(hidden_states, token_idx=-1, models=models):\n",
    "    # hidden_states is a dictionary with a key for each model, where its value is a list of hidden states for all batches  \n",
    "    \n",
    "    # compute input correlation matrices within each layer\n",
    "    cor_mtxs = {m: [] for m in models}\n",
    "    for model in models:\n",
    "        for layer in tqdm(range(len(hidden_states[model][0]))):\n",
    "            h = torch.cat([\n",
    "                hs[layer][:,token_idx] for hs in hidden_states[model]\n",
    "            ], dim=0)\n",
    "            cor = correlation_mtx(h.T)\n",
    "            cor_mtxs[model].append(cor)\n",
    "            \n",
    "    # compute similarity between each layer of model1 and each layer of model2\n",
    "    layer_sizes = [len(cor_mtxs[m]) for m in models]\n",
    "    sims = torch.zeros(layer_sizes)\n",
    "    for l1 in tqdm(range(layer_sizes[0])):\n",
    "        c1 = cor_mtxs[models[0]][l1].reshape(-1)\n",
    "        c1 = (c1-c1.mean())/c1.std()\n",
    "        for l2 in range(layer_sizes[1]):\n",
    "            c2 = cor_mtxs[models[1]][l2].reshape(-1)\n",
    "            c2 = (c2-c2.mean())/c2.std()\n",
    "            sims[l1,l2] = ( c1*c2 ).mean() \n",
    "\n",
    "    return sims, cor_matxs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d9af33d-ce41-432c-ab30-a111d561b50c",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "rsa(hidden_states)\n",
    "plt.imshow(sims.numpy())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "568ee5c3-9bc4-4f3b-a924-6402185be186",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Other stuff (tbd)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "79caade7",
   "metadata": {},
   "outputs": [],
   "source": [
    "set_seed(42)\n",
    "\n",
    "###################\n",
    "# data loaders\n",
    "###################\n",
    "raw_data = bound_alignment_sampler(\n",
    "    tokenizer, 10000, [lower_bound_alignment_example_sampler]\n",
    ")\n",
    "\n",
    "raw_train = (\n",
    "    raw_data[0][:8000],\n",
    "    raw_data[1][:8000],\n",
    "    raw_data[2][:8000],\n",
    "    raw_data[3][:8000],\n",
    ")\n",
    "raw_eval = (\n",
    "    raw_data[0][8000:9000],\n",
    "    raw_data[1][8000:9000],\n",
    "    raw_data[2][8000:9000],\n",
    "    raw_data[3][8000:9000],\n",
    ")\n",
    "raw_test = (\n",
    "    raw_data[0][9000:],\n",
    "    raw_data[1][9000:],\n",
    "    raw_data[2][9000:],\n",
    "    raw_data[3][9000:],\n",
    ")\n",
    "train_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_train[0],\n",
    "        \"source_input_ids\": raw_train[1],\n",
    "        \"labels\": raw_train[2],\n",
    "        \"intervention_ids\": raw_train[3],  # we will not use this field\n",
    "    }\n",
    ").with_format(\"torch\")\n",
    "train_dataloader = DataLoader(\n",
    "    train_dataset,\n",
    "    batch_size=16,\n",
    ")\n",
    "eval_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_eval[0],\n",
    "        \"source_input_ids\": raw_eval[1],\n",
    "        \"labels\": raw_eval[2],\n",
    "        \"intervention_ids\": raw_eval[3],  # we will not use this field\n",
    "    }\n",
    ").with_format(\"torch\")\n",
    "eval_dataloader = DataLoader(\n",
    "    eval_dataset,\n",
    "    batch_size=16,\n",
    ")\n",
    "test_dataset = Dataset.from_dict(\n",
    "    {\n",
    "        \"input_ids\": raw_test[0],\n",
    "        \"source_input_ids\": raw_test[1],\n",
    "        \"labels\": raw_test[2],\n",
    "        \"intervention_ids\": raw_test[3],  # we will not use this field\n",
    "    }\n",
    ").with_format(\"torch\")\n",
    "test_dataloader = DataLoader(\n",
    "    test_dataset,\n",
    "    batch_size=16,\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "47e83259",
   "metadata": {},
   "source": [
    "### Boundless DAS on Position-aligned Tokens"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "296d0a11",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simple_boundless_das_position_config(model_type, intervention_type, layer):\n",
    "    config = IntervenableConfig(\n",
    "        model_type=model_type,\n",
    "        representations=[\n",
    "            RepresentationConfig(\n",
    "                layer,              # layer\n",
    "                intervention_type,  # intervention type\n",
    "            ),\n",
    "        ],\n",
    "        intervention_types=BoundlessRotatedSpaceIntervention,\n",
    "    )\n",
    "    return config\n",
    "\n",
    "\n",
    "config = simple_boundless_das_position_config(\n",
    "    type(llama), \"block_output\", 15\n",
    ")\n",
    "intervenable = IntervenableModel(config, llama)\n",
    "intervenable.set_device(\"cuda\")\n",
    "intervenable.disable_model_gradients()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "740e3724",
   "metadata": {},
   "outputs": [],
   "source": [
    "t_total = int(len(train_dataloader) * 3)\n",
    "warm_up_steps = 0.1 * t_total\n",
    "optimizer_params = []\n",
    "for k, v in intervenable.interventions.items():\n",
    "    optimizer_params += [{\"params\": v[0].rotate_layer.parameters()}]\n",
    "    optimizer_params += [{\"params\": v[0].intervention_boundaries, \"lr\": 1e-2}]\n",
    "optimizer = torch.optim.Adam(optimizer_params, lr=1e-3)\n",
    "scheduler = get_linear_schedule_with_warmup(\n",
    "    optimizer, num_warmup_steps=warm_up_steps, num_training_steps=t_total\n",
    ")\n",
    "\n",
    "\n",
    "# You can define your custom compute_metrics function.\n",
    "def compute_metrics(eval_preds, eval_labels):\n",
    "    total_count = 0\n",
    "    correct_count = 0\n",
    "    for eval_pred, eval_label in zip(eval_preds, eval_labels):\n",
    "        actual_test_labels = eval_label[:, -1]\n",
    "        pred_test_labels = torch.argmax(eval_pred[:, -1], dim=-1)\n",
    "        correct_labels = actual_test_labels == pred_test_labels\n",
    "        total_count += len(correct_labels)\n",
    "        correct_count += correct_labels.sum().tolist()\n",
    "    accuracy = round(correct_count / total_count, 2)\n",
    "    return {\"accuracy\": accuracy}\n",
    "\n",
    "\n",
    "epochs = 3\n",
    "gradient_accumulation_steps = 4\n",
    "total_step = 0\n",
    "target_total_step = len(train_dataloader) * epochs\n",
    "temperature_start = 50.0\n",
    "temperature_end = 0.1\n",
    "temperature_schedule = (\n",
    "    torch.linspace(temperature_start, temperature_end, target_total_step)\n",
    "    .to(torch.bfloat16)\n",
    "    .to(\"cuda\")\n",
    ")\n",
    "intervenable.set_temperature(temperature_schedule[total_step])\n",
    "\n",
    "\n",
    "def calculate_loss(logits, labels):\n",
    "    shift_logits = logits[..., :, :].contiguous()\n",
    "    shift_labels = labels[..., :].contiguous()\n",
    "    # Flatten the tokens\n",
    "    loss_fct = CrossEntropyLoss()\n",
    "    shift_logits = shift_logits.view(-1, intervenable.model_config.vocab_size)\n",
    "    shift_labels = shift_labels.view(-1)\n",
    "    # Enable model parallelism\n",
    "    shift_labels = shift_labels.to(shift_logits.device)\n",
    "    loss = loss_fct(shift_logits, shift_labels)\n",
    "\n",
    "    for k, v in intervenable.interventions.items():\n",
    "        boundary_loss = 1.0 * v[0].intervention_boundaries.sum()\n",
    "    loss += boundary_loss\n",
    "\n",
    "    return loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5cc2a247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "llama trainable parameters:  0\n",
      "intervention trainable parameters:  16777218\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch: 0: 100%|██████████████████████████████████████████████████████████████████████████████████████| 500/500 [07:05<00:00,  1.18it/s, loss=0.5, acc=0.88]\n",
      "Epoch: 1: 100%|█████████████████████████████████████████████████████████████████████████████████████| 500/500 [07:58<00:00,  1.04it/s, loss=0.39, acc=0.94]\n",
      "Epoch: 2: 100%|█████████████████████████████████████████████████████████████████████████████████████| 500/500 [08:19<00:00,  1.00it/s, loss=0.35, acc=0.94]\n",
      "Epoch: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 3/3 [23:23<00:00, 467.83s/it]\n"
     ]
    }
   ],
   "source": [
    "intervenable.model.train()  # train enables drop-off but no grads\n",
    "print(\"llama trainable parameters: \", count_parameters(intervenable.model))\n",
    "print(\"intervention trainable parameters: \", intervenable.count_parameters())\n",
    "train_iterator = trange(0, int(epochs), desc=\"Epoch\")\n",
    "for epoch in train_iterator:\n",
    "    epoch_iterator = tqdm(\n",
    "        train_dataloader, desc=f\"Epoch: {epoch}\", position=0, leave=True\n",
    "    )\n",
    "    for step, inputs in enumerate(epoch_iterator):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(\"cuda\")\n",
    "        b_s = inputs[\"input_ids\"].shape[0]\n",
    "        _, counterfactual_outputs = intervenable(\n",
    "            {\"input_ids\": inputs[\"input_ids\"]},\n",
    "            [{\"input_ids\": inputs[\"source_input_ids\"]}],\n",
    "            {\"sources->base\": 80},  # swap 80th token\n",
    "        )\n",
    "        eval_metrics = compute_metrics(\n",
    "            [counterfactual_outputs.logits], [inputs[\"labels\"]]\n",
    "        )\n",
    "\n",
    "        # loss and backprop\n",
    "        loss = calculate_loss(counterfactual_outputs.logits, inputs[\"labels\"])\n",
    "        loss_str = round(loss.item(), 2)\n",
    "        epoch_iterator.set_postfix({\"loss\": loss_str, \"acc\": eval_metrics[\"accuracy\"]})\n",
    "\n",
    "        if gradient_accumulation_steps > 1:\n",
    "            loss = loss / gradient_accumulation_steps\n",
    "        loss.backward()\n",
    "        if total_step % gradient_accumulation_steps == 0:\n",
    "            if not (gradient_accumulation_steps > 1 and total_step == 0):\n",
    "                optimizer.step()\n",
    "                scheduler.step()\n",
    "                intervenable.set_zero_grad()\n",
    "                intervenable.set_temperature(temperature_schedule[total_step])\n",
    "        total_step += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3323b113",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Test: 100%|████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 63/63 [00:45<00:00,  1.38it/s]"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'accuracy': 0.96}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "# evaluation on the test set\n",
    "eval_labels = []\n",
    "eval_preds = []\n",
    "with torch.no_grad():\n",
    "    epoch_iterator = tqdm(test_dataloader, desc=f\"Test\")\n",
    "    for step, inputs in enumerate(epoch_iterator):\n",
    "        for k, v in inputs.items():\n",
    "            if v is not None and isinstance(v, torch.Tensor):\n",
    "                inputs[k] = v.to(\"cuda\")\n",
    "        b_s = inputs[\"input_ids\"].shape[0]\n",
    "        _, counterfactual_outputs = intervenable(\n",
    "            {\"input_ids\": inputs[\"input_ids\"]},\n",
    "            [{\"input_ids\": inputs[\"source_input_ids\"]}],\n",
    "            {\"sources->base\": 80},  # swap 80th token\n",
    "        )\n",
    "        eval_labels += [inputs[\"labels\"]]\n",
    "        eval_preds += [counterfactual_outputs.logits]\n",
    "eval_metrics = compute_metrics(eval_preds, eval_labels)\n",
    "print(eval_metrics)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbd6296",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
